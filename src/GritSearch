pip install transformers[torch] 
pip install datasets
pip install pandas
pip install scikit-learn
pip install optuna

import pandas as pd
import torch
import numpy as np
import optuna
from torch.utils.data import TensorDataset, random_split
from transformers import (
    AutoTokenizer,
    BertPreTrainedModel,
    BertModel,
    BertConfig,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)
import torch.nn as nn
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

import pandas as pd
df = pd.read_csv('Aspect_Based.csv', sep=';')

# Mendefinisikan tokenizer dari IndoBERT
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p2")

# Membuat mapping untuk aspek dan sentimen
unique_aspects = sorted(list(df['Aspect'].unique()))
aspect_map = {aspect: i for i, aspect in enumerate(unique_aspects)}
unique_sentiments = sorted(list(df['Sentiment'].unique()))
sentiment_map = {sentiment: i for i, sentiment in enumerate(unique_sentiments)}

# Fungsi untuk memproses data mentah menjadi format tensor
def process_data(dataframe, tokenizer, aspect_map, max_len=256):
    input_ids = []
    attention_masks = []
    aspect_labels = []
    sentiment_labels = []

    for _, row in dataframe.iterrows():
        text = str(row['Review'])
        aspect = str(row['Aspect'])
        sentiment = int(row['Sentiment'])

        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
        aspect_labels.append(aspect_map.get(aspect, -1))
        sentiment_labels.append(sentiment_map.get(sentiment, -1))

    return {
        'input_ids': torch.cat(input_ids, dim=0),
        'attention_mask': torch.cat(attention_masks, dim=0),
        'aspect_labels': torch.tensor(aspect_labels, dtype=torch.long),
        'sentiment_labels': torch.tensor(sentiment_labels, dtype=torch.long)
    }

processed_data = process_data(df, tokenizer, aspect_map)
dataset = TensorDataset(
    processed_data['input_ids'],
    processed_data['attention_mask'],
    processed_data['aspect_labels'],
    processed_data['sentiment_labels']
)

# Pembagian data 70:15:15 (train, val, test)
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])
print(f"Data dibagi menjadi: {len(train_dataset)} train, {len(val_dataset)} validation, {len(test_dataset)} test.")

# Mendefinisikan kelas arsitektur model kustom
class IndoBERTABSA(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_aspect_labels = len(aspect_map)
        self.num_sentiment_labels = len(sentiment_map)
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.aspect_classifier = nn.Linear(config.hidden_size, self.num_aspect_labels)
        self.sentiment_classifier = nn.Linear(config.hidden_size, self.num_sentiment_labels)
        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        # Untuk integrasi dengan Trainer, kita gabungkan label
        # Trainer akan memprosesnya dengan benar
        aspect_labels = labels[:, 0]
        sentiment_labels = labels[:, 1]

        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        pooled_output = self.dropout(outputs[1])
        aspect_logits = self.aspect_classifier(pooled_output)
        sentiment_logits = self.sentiment_classifier(pooled_output)

        loss = None
        if aspect_labels is not None and sentiment_labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            aspect_loss = loss_fct(aspect_logits.view(-1, self.num_aspect_labels), aspect_labels.view(-1))
            sentiment_loss = loss_fct(sentiment_logits.view(-1, self.num_sentiment_labels), sentiment_labels.view(-1))
            loss = aspect_loss + sentiment_loss # Total loss

        return (loss, (aspect_logits, sentiment_logits)) if loss is not None else (aspect_logits, sentiment_logits)


# Fungsi untuk menginisialisasi model di setiap trial
def model_init():
    config = BertConfig.from_pretrained(
        "indobenchmark/indobert-base-p2",
        num_labels_aspect=len(aspect_map),
        num_labels_sentiment=len(sentiment_map)
    )
    return IndoBERTABSA(config)

# Fungsi untuk menghitung metrik (akurasi dan F1-score)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    aspect_logits, sentiment_logits = logits

    # Pisahkan label untuk aspek dan sentimen
    aspect_labels = labels[:, 0]
    sentiment_labels = labels[:, 1]

    # Dapatkan prediksi dengan mengambil argmax
    aspect_preds = np.argmax(aspect_logits, axis=-1)
    sentiment_preds = np.argmax(sentiment_logits, axis=-1)

    # Hitung metrik untuk Aspek
    aspect_precision, aspect_recall, aspect_f1, _ = precision_recall_fscore_support(
        aspect_labels, aspect_preds, average='weighted', zero_division=0
    )
    aspect_accuracy = accuracy_score(aspect_labels, aspect_preds)

    # Hitung metrik untuk Sentimen
    sentiment_precision, sentiment_recall, sentiment_f1, _ = precision_recall_fscore_support(
        sentiment_labels, sentiment_preds, average='weighted', zero_division=0
    )
    sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_preds)

    # Kembalikan semua metrik dalam sebuah dictionary
    return {
        'aspect_accuracy': aspect_accuracy,
        'aspect_precision': aspect_precision,
        'aspect_recall': aspect_recall,
        'aspect_f1': aspect_f1,
        'sentiment_accuracy': sentiment_accuracy,
        'sentiment_precision': sentiment_precision,
        'sentiment_recall': sentiment_recall,
        'sentiment_f1': sentiment_f1,
        'combined_f1': (aspect_f1 + sentiment_f1) / 2
    }


# Custom collator untuk menyatukan label
def collate_fn(batch):
    input_ids = torch.stack([item[0] for item in batch])
    attention_mask = torch.stack([item[1] for item in batch])
    aspect_labels = torch.stack([item[2] for item in batch])
    sentiment_labels = torch.stack([item[3] for item in batch])
    # Gabungkan label menjadi satu tensor [batch_size, 2]
    labels = torch.stack([aspect_labels, sentiment_labels], dim=1)
    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}

# Mengatur level logging Optuna
import optuna
import logging
optuna.logging.set_verbosity(optuna.logging.INFO)

# Fungsi yang mendefinisikan "grid" hyperparameter untuk diuji
def hp_space(trial: optuna.Trial):
    params = {
        "learning_rate": trial.suggest_categorical("learning_rate", [2e-5, 3e-5, 5e-5, 7e-5]),
        "num_train_epochs": trial.suggest_categorical("num_train_epochs", [10]),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32]),
        "weight_decay": trial.suggest_categorical("weight_decay", [1e-3, 5e-3, 1e-2, 2e-2, 5e-2]),
    }

    print(f"\n---> [TRIAL #{trial.number}] Mengajukan kombinasi: {params}")

    return params

# Konfigurasi dasar untuk Trainer
training_args = TrainingArguments(
    output_dir="./grid_search_results",
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch",
    disable_tqdm=False,
    logging_dir="./logs",
    load_best_model_at_end=True,
    report_to="none"
)

# Membuat objek Trainer
trainer = Trainer(
    args=training_args,
    model_init=model_init,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    data_collator=collate_fn,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Memulai proses pencarian!
best_run = trainer.hyperparameter_search(
    hp_space=hp_space,
    n_trials=10, # Jumlah kombinasi yang akan dicoba
    direction="maximize",
    compute_objective=lambda metrics: metrics["eval_combined_f1"],
)
print("\n--- PENCARIAN SELESAI ---")
print("Kombinasi hyperparameter terbaik ditemukan:")
for param, value in best_run.hyperparameters.items():
    print(f"- {param}: {value}")

print(f"\nSkor F1 Gabungan Terbaik: {best_run.objective:.4f}")
